{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Whitebox Misclassification Attack Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "This cell imports all the necessary modules from the `adversarial_lab` library, as well as other common libraries like TensorFlow, NumPy, and PIL for image manipulation and numerical operations.\n",
    "\n",
    "Key `adversarial_lab` modules used:\n",
    "* `PGD`: Projected Gradient Descent optimizer used to generate adversarial noise.\n",
    "* `CategoricalCrossEntropy`: Loss function to guide the optimization.\n",
    "* `WhiteBoxMisclassification`: The main attacker class.\n",
    "* `AdditiveNoiseGenerator`: Generates the initial noise (can be zeros, random, etc.).\n",
    "* `PreprocessingFromFunction`: Wraps a custom preprocessing function.\n",
    "* `POClip`, `PONoisedSampleBounding`: Constraints applied to the noise or the noisy sample.\n",
    "* `Plotting`: Utility for visualizing images and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversarial_lab.core.optimizers import PGD\n",
    "from adversarial_lab.callbacks import EarlyStopping\n",
    "from adversarial_lab.core.losses import CategoricalCrossEntropy\n",
    "from adversarial_lab.attacker.whitebox import WhiteBoxMisclassification\n",
    "from adversarial_lab.core.noise_generators import AdditiveNoiseGenerator\n",
    "from adversarial_lab.core.preprocessing import PreprocessingFromFunction\n",
    "from adversarial_lab.core.constraints import POClip, PONoisedSampleBounding\n",
    "\n",
    "from adversarial_lab.utils import Plotting\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Loading\n",
    "\n",
    "Here, you can choose which pre-trained model to attack. The supported models are `InceptionV3`, `ResNet50`, and `MobileNetV2`.\n",
    "The subsequent code cell will load the chosen Keras application model with pre-trained ImageNet weights and set the appropriate `input_shape` and model-specific `preprocess_input` and `decode_predictions` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"InceptionV3\"       # Supported models: InceptionV3, ResNet50, MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == \"InceptionV3\":\n",
    "    from tensorflow.keras.applications import InceptionV3\n",
    "    from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "    model = InceptionV3(weights='imagenet')\n",
    "    input_shape = (299, 299, 3)\n",
    "elif MODEL == \"ResNet50\":\n",
    "    from tensorflow.keras.applications import ResNet50\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    input_shape = (224, 224, 3)\n",
    "elif MODEL == \"MobileNetV2\":\n",
    "    from tensorflow.keras.applications import MobileNetV2\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "    model = MobileNetV2(weights='imagenet')\n",
    "    input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Preprocessing Function\n",
    "\n",
    "Neural network models expect input data in a specific format. This `preprocess` function handles:\n",
    "* **Type Casting**: Converts the input image data to `float32`.\n",
    "* **Grayscale to RGB**: Converts grayscale images (2D or 3D with 1 channel) to 3-channel RGB format, as expected by the pre-trained models.\n",
    "* **Resizing**: Resizes the image to the model's required input dimensions (e.g., (299, 299) for InceptionV3).\n",
    "* **Batch Dimension**: Adds a batch dimension to the image tensor, as models typically expect batches of images.\n",
    "* **Model-Specific Preprocessing**: Applies the specific preprocessing steps required by the chosen Keras model (e.g., scaling pixel values to a certain range like [-1, 1] or [0, 1], or BGR conversion if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample, *args, **kwargs):\n",
    "    input_sample = tf.cast(sample, dtype=tf.float32)\n",
    "    if len(input_sample.shape) == 2:\n",
    "        input_sample = tf.expand_dims(input_sample, axis=-1)\n",
    "        input_sample = tf.image.grayscale_to_rgb(input_sample)\n",
    "\n",
    "    elif len(input_sample.shape) == 3 and input_sample.shape[-1] == 1:\n",
    "        input_sample = tf.image.grayscale_to_rgb(input_sample)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(input_sample, dtype=tf.float32)\n",
    "    resized_image = tf.image.resize(input_tensor, (299, 299))\n",
    "    batch_image = tf.expand_dims(resized_image, axis=0)\n",
    "    return preprocess_input(batch_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Classify Original Image\n",
    "\n",
    "This cell loads an example image (`panda.jpg`). We then preprocess it and get the model's prediction on this original image. This establishes a baseline classification before applying any adversarial attack. The top predicted classes and their probabilities are displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('data/panda.jpg')\n",
    "image_array = np.array(image)\n",
    "\n",
    "predictions = model.predict(preprocess(image_array), verbose=0)\n",
    "print(\"Predicted class:\", decode_predictions(predictions, top=1)[0][0][1])\n",
    "print(\"Predicted class index:\", np.argmax(predictions, axis=1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure the Adversarial Attack Components\n",
    "\n",
    "Now, we set up the components required for the `WhiteBoxMisclassification` attacker:\n",
    "* **Loss Function**: `CategoricalCrossEntropy` is used. For misclassification, the goal is to maximize this loss with respect to the original class or minimize it towards a target class.\n",
    "* **Optimizer**: `PGD` (Projected Gradient Descent) is chosen. This is an iterative optimization algorithm that takes steps in the direction of the gradient of the loss function and then projects the resulting perturbation back onto a constrained space (e.g., an L-infinity ball). The `learning_rate` (or step size) for PGD is set here.\n",
    "* **Constraints**:\n",
    "    * `POClip`: This \"Perturbation Operation\" clips the generated noise values to a specific range (e.g., between -2 and +2). This limits the magnitude of individual pixel changes in the noise.\n",
    "    * `PONoisedSampleBounding`: This ensures that the pixel values of the *adversarial image* (original image + noise) remain within a valid range (e.g., 0 to 255 for standard images).\n",
    "* **Noise Generator**: `AdditiveNoiseGenerator` is used.\n",
    "    * `dist=\"zeros\"`: Initializes the noise with all zeros. The PGD optimizer will then iteratively modify this noise.\n",
    "    * `scale=[0, 255]`: Informs the generator about the typical range of the input data it will be added to. This is more relevant if a random distribution (like \"uniform\" or \"normal\") was used for `dist`. For `dist=\"zeros\"`, `PONoisedSampleBounding` is key.\n",
    "* **Preprocessing Wrapper**: `PreprocessingFromFunction.create(preprocess)` wraps our custom `preprocess` function so it can be seamlessly integrated into the attacker's workflow. The attacker will use this to preprocess images before feeding them to the model during the attack process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CategoricalCrossEntropy()\n",
    "optimizer = PGD(learning_rate=1.0)\n",
    "\n",
    "constrain1 = POClip(min=-2, max=2)\n",
    "constrain2 = PONoisedSampleBounding(min=0, max=255)\n",
    "constraints = [constrain1, constrain2]\n",
    "\n",
    "noise_generator = AdditiveNoiseGenerator(scale=[0, 255], dist=\"zeros\")\n",
    "preprocessing = PreprocessingFromFunction.create(preprocess)\n",
    "\n",
    "early_stopping_non_targetted = EarlyStopping(trigger=\"misclassification\")\n",
    "early_stopping_targetted = EarlyStopping(trigger=\"misclassification\", target_class=924, confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize the Attacker\n",
    "\n",
    "With all components defined, we initialize the `WhiteBoxMisclassification` attacker.\n",
    "* `model`: The target neural network.\n",
    "* `optimizer`: The PGD optimizer configured above.\n",
    "* `loss`: The categorical cross-entropy loss function.\n",
    "* `noise_generator`: The additive noise generator.\n",
    "* `constraints`: The list of constraints to apply during noise generation.\n",
    "* `preprocessing`: The wrapped preprocessing function.\n",
    "* `verbose`: Controls the amount of logging output during the attack (0: silent, 1: progress bar, 2: epoch results, 3: detailed step results).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker = WhiteBoxMisclassification(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    noise_generator=noise_generator,\n",
    "    constraints=constraints,\n",
    "    preprocessing=preprocessing,\n",
    "    callbacks=[early_stopping_targetted],\n",
    "    verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Perform a Targeted Misclassification Attack\n",
    "\n",
    "In a targeted attack, we aim to make the model classify the input image as a *specific* incorrect class.\n",
    "* `image_array`: The original input image as a NumPy array.\n",
    "* `target_class`: The integer index of the target class we want the model to predict. For ImageNet, class 924 is \"sports_car\". You can find other ImageNet class IDs online or by exploring `decode_predictions` output for various images.\n",
    "* `epochs`: The number of iterations the PGD optimizer will run to craft the adversarial noise.\n",
    "* `on_original=True`: Specifies that the attack generates noise to be added to the *original, unnormalized* image. If `False`, it would expect the input `image_array` to be already preprocessed and generate noise accordingly.\n",
    "\n",
    "The `attacker.attack()` method returns the generated adversarial noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise, noise_meta = attacker.attack(image_array, \n",
    "                        target_class=924,\n",
    "                        epochs=20, \n",
    "                        on_original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Targeted Attack Results and Verify\n",
    "\n",
    "We use `Plotting.plot_images_and_noise` to display:\n",
    "1.  The original image.\n",
    "2.  The generated adversarial noise (magnified for visibility, as it's often subtle).\n",
    "3.  The resulting adversarial image (original + noise).\n",
    "\n",
    "We then preprocess the adversarial image and feed it to the model to see if the attack was successful in forcing the targeted misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting.plot_images_and_noise(image_array, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Perform an Untargeted Misclassification Attack (Random Strategy)\n",
    "\n",
    "In an untargeted attack, the goal is to make the model misclassify the image as *any* class other than the original correct class.\n",
    "* `strategy=\"random\"`: This tells the attacker to randomly pick a class (that is not the original true class) as the misclassification target for each iteration or for the overall attack. The library might also support other strategies like \"least_likely\" (targeting the class the model deems least probable for the original image).\n",
    "* `target_class` is omitted when using a strategy like \"random\" as the attacker determines the target internally.\n",
    "* `original_class_index` (calculated in Cell 4) is implicitly used by the attacker with `strategy=\"random\"` to ensure the random target is not the original class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker = WhiteBoxMisclassification(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    noise_generator=noise_generator,\n",
    "    constraints=constraints,\n",
    "    preprocessing=preprocessing,\n",
    "    callbacks=[early_stopping_non_targetted],\n",
    "    verbose=3)\n",
    "\n",
    "noise, noise_meta = attacker.attack(image_array, \n",
    "                        epochs=20, \n",
    "                        strategy=\"random\",\n",
    "                        on_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting.plot_images_and_noise(image_array, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook demonstrated how to perform basic white-box targeted and untargeted misclassification attacks using the `adversarial_lab` library. We saw how relatively small, often imperceptible, perturbations can drastically change a deep neural network's predictions.\n",
    "\n",
    "**Further Exploration:**\n",
    "* Try different models (`ResNet50`, `MobileNetV2`).\n",
    "* Experiment with different `learning_rate` values for PGD and `epochs`.\n",
    "* Adjust the `POClip` constraint value for the noise.\n",
    "* Test with different images.\n",
    "* Explore other attack types or optimizers if available in the library.\n",
    "* Investigate other untargeted strategies like \"least_likely\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
